#!/usr/bin/env python

"""Web scraper application for supost.com.

This program searches supost.com for posts that contain certain given keywords.

Example:
    $ python3 supost_web_scraper.py

Attributes:
    OFFSET_INCREASE (int): number of pages to search per iteration in the
        program loop.
"""

import datetime                       # for date and time
import httplib2                     # for receiving web pages
import requests
import time
import json
import logging
from bs4 import BeautifulSoup        # for getting information from web pages

__author__ = "Derin Dutz, Kevin Ko"
__copyright__ = "Copyright 2016, Derin Dutz. All rights reserved."
__credits__ = ["Derin Dutz", "Kevin Ko"]
__license__ = "MIT"
__version__ = "1.0.1"
__status__ = "Development"

KEYWORDS = ["tree"]
OFFSET_INCREASE = 99  # amount to increase page offset
SLEEP_TIME = 30
SENT_POSTS = []

# twilio constants
TWILIO_ACCOUNT_SID = "AC8b6d0bd0dfc3c866e6218f6109b37761"
TWILIO_AUTH_TOKEN = "0a2d60e6e9d3794ac633b19de60e1932"
MY_PHONE_NUMBER = "+16501234567" # CHANGE ME!
TWILIO_PHONE_NUMBER = "+16506145443"

logging.basicConfig(format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%m-%d-%Y,%H:%M:%S', level=logging.INFO)
logger = logging.getLogger(__name__)
   
def main():
    """Runs the program."""
    while True:
    	logger.info("Scraping data from supost.com for keywords: {0}...".format(", ".join(KEYWORDS)))
    	matches = scrape_supost(KEYWORDS, 1)
    	logger.info("Done scraping.")
    	if len(matches) > 0:
    		notify_sms(KEYWORDS, matches)
    	time.sleep(SLEEP_TIME)


def scrape_supost(keywords, days_to_check):
    """Scrapes supost.com to find all posts which contains the given keywords.

    Args:
        keywords (list of str): keywords to search for.
        days_to_check (int): number of days back to search.

    Returns:
        A list of matches where each match is a tuple of keyword, link, and
        post title.
    """
    oldest_date = (datetime.date.today() -
                   datetime.timedelta(days=days_to_check))
    oldest_date_str = oldest_date.strftime("%a, %b %d")

    offset = 0
    matches = []
    link = "http://supost.com/search/index/5"

    while True:
        response, content = httplib2.Http().request(link)
        index_page = BeautifulSoup(content,"html.parser")
        for link in index_page.find_all("a"):
            href = link.get("href").encode('utf-8')
            if ("post/index" in href):
                matches.extend(
                    scrape_post("http://supost.com" + href, keywords))

        # stops scraper when oldest date is found
        if (oldest_date_str in index_page.get_text().encode('utf-8')):
            return matches

        offset += OFFSET_INCREASE

        # updates the link with the new offset
        link = "http://supost.com/search/index/5?offset=" + str(offset)


def scrape_post(link, keywords):
    """Scrapes the post corresponding to the given link and searches for the
    given keywords.

    Args:
        link (str): link to scrape.
        keywords (list of str): keywords to search for.

    Returns:
        A list of matches where each match is a tuple of keyword, link, and
        post title.
    """
    # print("scraping post: " + link + "...")
    matches = []
    response, content = httplib2.Http().request(link)
    post_page = BeautifulSoup(content, "html.parser")
    for keyword in keywords:
        if keyword in post_page.get_text().encode('utf-8').lower():
            post_title = post_page.find("h2", {"id": "posttitle"}).text
            matches.append((keyword, link, post_title))
    return matches


def notify_sms(keywords, matches):
	"""
	Sends SMS via Twilio notification to number defined at MY_PHONE_NUMBER with the newly
	found posts matching keywords on SUPost. If the post has already been sent a notification
	during the runtime of this script then no further notifications about the same post are sent.
	
	Args:
		matches (list of tuples): posts found
		keywords (list of str): keywords to search for
	
	"""
	tosend = []
	for match in matches:
		if match[1] not in SENT_POSTS:
			tosend.append(match[1])
			SENT_POSTS.append(match[1])
	
	if len(tosend) > 0:
		logger.info("Found {0} new post(s) to update my scraper owner!".format(len(tosend)))
		body = "From your very own SUPost scraper. Found {0} post(s) about {1}:\n\n".format(len(tosend), ", ".join(keywords))
		body += "\n".join(tosend)
		endpoint = "https://api.twilio.com/2010-04-01/Accounts/{0}/Messages.json".format(TWILIO_ACCOUNT_SID)
		data = {"To": MY_PHONE_NUMBER,
				"From": TWILIO_PHONE_NUMBER,
				"Body": body}
		r = requests.post(endpoint, auth=(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN), data=data)
		try:
			tr = json.loads(r.text)
			if tr["status"] == "failed" or tr["status"] == "undelivered":
				logger.error("Unable to send text message: Twilio reported failed or undelivered.")
		except Exception, e:
			logger.error("Unable to send text message: Invalid response generated by Twilio API.")

if __name__ == "__main__":
    main()
